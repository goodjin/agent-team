# Task 1: æ›´æ–° LLM é…ç½®ç³»ç»Ÿ

**ä¼˜å…ˆçº§**: P0
**é¢„è®¡å·¥æ—¶**: 4 å°æ—¶
**ä¾èµ–**: æ— 
**çŠ¶æ€**: å¾…æ‰§è¡Œ

---

## ç›®æ ‡

1. æ›´æ–° `config/llm.yaml` é…ç½®æ–‡ä»¶ï¼Œå¢åŠ æƒé‡é…ç½®
2. æ·»åŠ  MiniMax å’Œ BigModel æœåŠ¡å•†é…ç½®
3. å®ç°æƒé‡é€‰æ‹©é€»è¾‘
4. å®ç°é…ç½®éªŒè¯è„šæœ¬

---

## è¾“å…¥

- ç°æœ‰é…ç½®æ–‡ä»¶ï¼š`config/llm.yaml`
- éœ€æ±‚æ–‡æ¡£ï¼š`docs/v5/03-llm-providers.md`

---

## è¾“å‡º

- æ›´æ–°åçš„é…ç½®æ–‡ä»¶ï¼š`config/llm.yaml`
- é…ç½®éªŒè¯è„šæœ¬ï¼š`scripts/validate-llm-config.ts`
- æ›´æ–°çš„é…ç½®ç®¡ç†å™¨ï¼š`src/services/llm-config.ts`

---

## å®ç°æ­¥éª¤

### æ­¥éª¤ 1: æ›´æ–°é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/llm.yaml`ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```yaml
version: "5.0.0"

# é»˜è®¤æœåŠ¡å•†
defaultProvider: "openai"

# æœåŠ¡å•†é…ç½®
providers:
  openai:
    name: "OpenAI"
    provider: "openai"
    apiKey: "${OPENAI_API_KEY}"
    baseURL: ""
    weight: 10
    enabled: true
    timeout: 60000
    maxRetries: 3
    models:
      gpt-4-turbo:
        model: "gpt-4-turbo-preview"
        maxTokens: 128000
        contextWindow: 128000
      gpt-4o:
        model: "gpt-4o"
        maxTokens: 128000
        contextWindow: 128000
      gpt-4o-mini:
        model: "gpt-4o-mini"
        maxTokens: 128000
        contextWindow: 128000

  claude:
    name: "Anthropic Claude"
    provider: "anthropic"
    apiKey: "${ANTHROPIC_API_KEY}"
    weight: 8
    enabled: true
    timeout: 60000
    maxRetries: 3
    models:
      claude-3-5-sonnet:
        model: "claude-3-5-sonnet-20241022"
        maxTokens: 200000
        contextWindow: 200000
      claude-3-5-haiku:
        model: "claude-3-5-haiku-20241022"
        maxTokens: 200000
        contextWindow: 200000

  deepseek:
    name: "DeepSeek"
    provider: "openai"
    apiKey: "${DEEPSEEK_API_KEY}"
    baseURL: "https://api.deepseek.com/v1"
    weight: 7
    enabled: true
    timeout: 60000
    maxRetries: 3
    models:
      deepseek-chat:
        model: "deepseek-chat"
        maxTokens: 64000
        contextWindow: 64000

  qwen:
    name: "Qwen (é€šä¹‰åƒé—®)"
    provider: "openai"
    apiKey: "${QWEN_API_KEY}"
    baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    weight: 6
    enabled: false
    timeout: 60000
    maxRetries: 3
    models:
      qwen-max:
        model: "qwen-max"
        maxTokens: 8000
        contextWindow: 30000
      qwen-turbo:
        model: "qwen-turbo"
        maxTokens: 8000
        contextWindow: 8000

  minimax:
    name: "MiniMax"
    provider: "openai"
    apiKey: "${MINIMAX_API_KEY}"
    baseURL: ""
    weight: 5
    enabled: false
    timeout: 60000
    maxRetries: 3
    models:
      minimax-m2-1:
        model: "MiniMax-M2.1"
        maxTokens: 204800
        contextWindow: 204800
      minimax-m2-1-lightning:
        model: "MiniMax-M2.1-lightning"
        maxTokens: 204800
        contextWindow: 204800

  bigmodel:
    name: "BigModel (æ™ºè°± GLM)"
    provider: "bigmodel"
    apiKey: "${BIGMODEL_API_KEY}"
    baseURL: "https://open.bigmodel.cn/api/paas/v4"
    weight: 5
    enabled: false
    timeout: 60000
    maxRetries: 3
    models:
      glm-4:
        model: "glm-4"
        maxTokens: 128000
        contextWindow: 128000
      glm-4-flash:
        model: "glm-4-flash"
        maxTokens: 128000
        contextWindow: 128000

# è§’è‰²ä¸“å±æœåŠ¡å•†é…ç½®
roleMapping:
  master-agent:
    provider: "claude"
    model: "claude-3-5-sonnet-20241022"

  developer:
    provider: "openai"
    model: "gpt-4-turbo-preview"

  tester:
    provider: "deepseek"
    model: "deepseek-chat"

  architect:
    provider: "claude"
    model: "claude-3-5-sonnet-20241022"
```

### æ­¥éª¤ 2: å®ç°é…ç½®ç®¡ç†å™¨

åˆ›å»ºæˆ–æ›´æ–° `src/services/llm-config.ts`ï¼š

```typescript
import fs from 'fs/promises';
import path from 'path';
import yaml from 'js-yaml';

export interface ModelConfig {
  model: string;
  maxTokens: number;
  contextWindow: number;
}

export interface ProviderConfig {
  name: string;
  provider: string;
  apiKey: string;
  baseURL?: string;
  weight: number;
  enabled: boolean;
  timeout: number;
  maxRetries: number;
  models: Record<string, ModelConfig>;
}

export interface RoleMappingConfig {
  provider: string;
  model: string;
}

export interface LLMConfig {
  version: string;
  defaultProvider: string;
  providers: Record<string, ProviderConfig>;
  roleMapping: Record<string, RoleMappingConfig>;
}

export class LLMConfigManager {
  private config: LLMConfig | null = null;

  async loadFromFile(filePath: string): Promise<void> {
    const content = await fs.readFile(filePath, 'utf-8');
    const rawConfig = yaml.load(content) as any;

    // æ›¿æ¢ç¯å¢ƒå˜é‡
    this.config = this.resolveEnvVars(rawConfig);
  }

  private resolveEnvVars(config: any): LLMConfig {
    const resolved = JSON.parse(
      JSON.stringify(config).replace(
        /\$\{([^}]+)\}/g,
        (_, key) => process.env[key] || ''
      )
    );
    return resolved;
  }

  /**
   * è·å–å¯ç”¨çš„æœåŠ¡å•†åˆ—è¡¨
   */
  getAvailableProviders(): ProviderConfig[] {
    if (!this.config) {
      throw new Error('Config not loaded');
    }

    return Object.values(this.config.providers).filter(
      (provider) =>
        provider.enabled &&
        provider.weight > 0 &&
        provider.apiKey !== ''
    );
  }

  /**
   * æŒ‰æƒé‡éšæœºé€‰æ‹©æœåŠ¡å•†
   */
  selectProvider(): ProviderConfig {
    const available = this.getAvailableProviders();

    if (available.length === 0) {
      throw new Error('No available providers');
    }

    // è®¡ç®—æ€»æƒé‡
    const totalWeight = available.reduce((sum, p) => sum + p.weight, 0);

    // éšæœºé€‰æ‹©
    let random = Math.random() * totalWeight;

    for (const provider of available) {
      random -= provider.weight;
      if (random <= 0) {
        return provider;
      }
    }

    // å…œåº•è¿”å›ç¬¬ä¸€ä¸ª
    return available[0];
  }

  /**
   * è·å–æŒ‡å®šæœåŠ¡å•†
   */
  getProvider(providerName: string): ProviderConfig | null {
    if (!this.config) {
      throw new Error('Config not loaded');
    }

    const provider = this.config.providers[providerName];

    if (!provider) {
      return null;
    }

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨
    if (!provider.enabled || provider.weight === 0 || provider.apiKey === '') {
      return null;
    }

    return provider;
  }

  /**
   * è·å–è§’è‰²ä¸“å±æœåŠ¡å•†
   */
  getProviderForRole(role: string): ProviderConfig | null {
    if (!this.config) {
      throw new Error('Config not loaded');
    }

    const mapping = this.config.roleMapping[role];
    if (!mapping) {
      return null;
    }

    return this.getProvider(mapping.provider);
  }

  /**
   * éªŒè¯é…ç½®
   */
  async validateConfig(): Promise<{
    valid: boolean;
    errors: string[];
    warnings: string[];
    summary: {
      totalProviders: number;
      enabledProviders: number;
      readyToUse: number;
    };
    providers: Array<{
      name: string;
      enabled: boolean;
      hasApiKey: boolean;
      weight: number;
      readyToUse: boolean;
    }>;
    recommendations: string[];
  }> {
    if (!this.config) {
      throw new Error('Config not loaded');
    }

    const errors: string[] = [];
    const warnings: string[] = [];
    const recommendations: string[] = [];
    const providerDetails: Array<any> = [];

    let totalProviders = 0;
    let enabledProviders = 0;
    let readyToUse = 0;

    for (const [key, provider] of Object.entries(this.config.providers)) {
      totalProviders++;

      const hasApiKey = provider.apiKey !== '';
      const isEnabled = provider.enabled;
      const hasWeight = provider.weight > 0;
      const isReady = isEnabled && hasApiKey && hasWeight;

      if (isEnabled) {
        enabledProviders++;
      }

      if (isReady) {
        readyToUse++;
      }

      providerDetails.push({
        name: provider.name,
        enabled: isEnabled,
        hasApiKey,
        weight: provider.weight,
        readyToUse: isReady,
      });

      // æ£€æŸ¥é”™è¯¯
      if (isEnabled && !hasApiKey) {
        warnings.push(`${provider.name}: enabled but missing API key`);
      }

      if (isEnabled && provider.weight === 0) {
        warnings.push(`${provider.name}: enabled but weight is 0`);
      }

      // æ£€æŸ¥æ¨¡å‹é…ç½®
      if (Object.keys(provider.models).length === 0) {
        errors.push(`${provider.name}: no models configured`);
      }
    }

    // å»ºè®®
    if (readyToUse === 0) {
      recommendations.push('No providers ready to use. Please configure at least one provider with API key.');
    } else if (readyToUse === 1) {
      recommendations.push('Only one provider available. Consider configuring backup providers.');
    }

    if (readyToUse < totalProviders / 2) {
      recommendations.push('More than half of providers are disabled. Consider enabling more providers for redundancy.');
    }

    return {
      valid: errors.length === 0 && readyToUse > 0,
      errors,
      warnings,
      summary: {
        totalProviders,
        enabledProviders,
        readyToUse,
      },
      providers: providerDetails,
      recommendations,
    };
  }
}
```

### æ­¥éª¤ 3: åˆ›å»ºéªŒè¯è„šæœ¬

åˆ›å»º `scripts/validate-llm-config.ts`ï¼š

```typescript
import { LLMConfigManager } from '../src/services/llm-config.js';
import path from 'path';

async function main() {
  const configPath = path.join(process.cwd(), 'config/llm.yaml');

  console.log('ğŸ” Validating LLM configuration...\n');
  console.log(`Config file: ${configPath}\n`);

  const manager = new LLMConfigManager();

  try {
    await manager.loadFromFile(configPath);
    console.log('âœ… Config file loaded successfully\n');
  } catch (error) {
    console.error('âŒ Failed to load config file:', error);
    process.exit(1);
  }

  const validation = await manager.validateConfig();

  // æ‰“å°æ‘˜è¦
  console.log('ğŸ“Š Summary:');
  console.log(`  Total providers: ${validation.summary.totalProviders}`);
  console.log(`  Enabled: ${validation.summary.enabledProviders}`);
  console.log(`  Ready to use: ${validation.summary.readyToUse}\n`);

  // æ‰“å°æœåŠ¡å•†è¯¦æƒ…
  console.log('ğŸ“‹ Providers:');
  validation.providers.forEach((p) => {
    const status = p.readyToUse ? 'âœ…' : 'âš ï¸';
    console.log(`  ${status} ${p.name}`);
    console.log(`     Enabled: ${p.enabled}, API Key: ${p.hasApiKey ? 'Yes' : 'No'}, Weight: ${p.weight}`);
  });
  console.log();

  // æ‰“å°é”™è¯¯
  if (validation.errors.length > 0) {
    console.log('âŒ Errors:');
    validation.errors.forEach((err) => console.log(`  - ${err}`));
    console.log();
  }

  // æ‰“å°è­¦å‘Š
  if (validation.warnings.length > 0) {
    console.log('âš ï¸  Warnings:');
    validation.warnings.forEach((warn) => console.log(`  - ${warn}`));
    console.log();
  }

  // æ‰“å°å»ºè®®
  if (validation.recommendations.length > 0) {
    console.log('ğŸ’¡ Recommendations:');
    validation.recommendations.forEach((rec) => console.log(`  - ${rec}`));
    console.log();
  }

  // æœ€ç»ˆç»“æœ
  if (validation.valid) {
    console.log('âœ… Configuration is valid and ready to use!\n');
    process.exit(0);
  } else {
    console.log('âŒ Configuration has issues. Please fix them before using.\n');
    process.exit(1);
  }
}

main();
```

### æ­¥éª¤ 4: æ›´æ–° package.json

æ·»åŠ éªŒè¯è„šæœ¬åˆ° `package.json`ï¼š

```json
{
  "scripts": {
    "validate-config": "tsx scripts/validate-llm-config.ts"
  }
}
```

---

## éªŒæ”¶æ ‡å‡†

- âœ… é…ç½®æ–‡ä»¶åŒ…å«æ‰€æœ‰æœåŠ¡å•†ï¼ˆOpenAI, Claude, DeepSeek, Qwen, MiniMax, BigModelï¼‰
- âœ… æ¯ä¸ªæœåŠ¡å•†éƒ½æœ‰ weight é…ç½®
- âœ… æƒé‡é€‰æ‹©é€»è¾‘æ­£ç¡®ï¼ˆæŒ‰æƒé‡æ¯”ä¾‹éšæœºé€‰æ‹©ï¼‰
- âœ… `weight: 0` æˆ– `apiKey` ä¸ºç©ºçš„æœåŠ¡å•†è¢«è¿‡æ»¤
- âœ… é…ç½®éªŒè¯è„šæœ¬å¯ä»¥æ­£å¸¸è¿è¡Œ

---

## æµ‹è¯•ç”¨ä¾‹

### æµ‹è¯• 1: æƒé‡é€‰æ‹©

```typescript
// æµ‹è¯•æƒé‡é€‰æ‹©é€»è¾‘
const manager = new LLMConfigManager();
await manager.loadFromFile('config/llm.yaml');

const selections = new Map<string, number>();

// è¿è¡Œ 1000 æ¬¡ï¼Œç»Ÿè®¡é€‰æ‹©åˆ†å¸ƒ
for (let i = 0; i < 1000; i++) {
  const provider = manager.selectProvider();
  selections.set(provider.name, (selections.get(provider.name) || 0) + 1);
}

// éªŒè¯åˆ†å¸ƒæ¥è¿‘æƒé‡æ¯”ä¾‹
console.log(selections);
// é¢„æœŸ: OpenAI ~40%, Claude ~32%, DeepSeek ~28%
```

### æµ‹è¯• 2: è¿‡æ»¤ä¸å¯ç”¨æœåŠ¡å•†

```typescript
// è®¾ç½®ç¯å¢ƒå˜é‡ä¸ºç©º
process.env.QWEN_API_KEY = '';

const manager = new LLMConfigManager();
await manager.loadFromFile('config/llm.yaml');

const available = manager.getAvailableProviders();

// éªŒè¯ Qwen ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­
const hasQwen = available.some(p => p.name.includes('Qwen'));
console.assert(!hasQwen, 'Qwen should not be available');
```

### æµ‹è¯• 3: é…ç½®éªŒè¯

```bash
npm run validate-config
```

é¢„æœŸè¾“å‡ºï¼š
```
ğŸ” Validating LLM configuration...

Config file: /path/to/config/llm.yaml

âœ… Config file loaded successfully

ğŸ“Š Summary:
  Total providers: 6
  Enabled: 3
  Ready to use: 3

ğŸ“‹ Providers:
  âœ… OpenAI
     Enabled: true, API Key: Yes, Weight: 10
  âœ… Anthropic Claude
     Enabled: true, API Key: Yes, Weight: 8
  âœ… DeepSeek
     Enabled: true, API Key: Yes, Weight: 7
  âš ï¸  Qwen (é€šä¹‰åƒé—®)
     Enabled: false, API Key: No, Weight: 6
  âš ï¸  MiniMax
     Enabled: false, API Key: No, Weight: 5
  âš ï¸  BigModel (æ™ºè°± GLM)
     Enabled: false, API Key: No, Weight: 5

âœ… Configuration is valid and ready to use!
```

---

## ç›¸å…³æ–‡æ¡£

- éœ€æ±‚æ–‡æ¡£ï¼š`docs/v5/01-requirements.md`
- æ¶æ„è®¾è®¡ï¼š`docs/v5/02-architecture.md`
- LLM é…ç½®è¯´æ˜ï¼š`docs/v5/03-llm-providers.md`
- ä»»åŠ¡æ‹†åˆ†ï¼š`docs/v5/04-task-breakdown.md`

---

## æ³¨æ„äº‹é¡¹

1. **ç¯å¢ƒå˜é‡**ï¼šAPI Key åº”è¯¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œä¸è¦ç¡¬ç¼–ç åœ¨é…ç½®æ–‡ä»¶ä¸­
2. **æƒé‡é…ç½®**ï¼šæƒé‡ä¸º 0 è¡¨ç¤ºæœåŠ¡å•†ä¸å¯ç”¨ï¼Œå³ä½¿ `enabled: true`
3. **ç©º API Key**ï¼šå¦‚æœ `apiKey` ä¸ºç©ºå­—ç¬¦ä¸²ï¼ŒæœåŠ¡å•†ä¹Ÿä¸å¯ç”¨
4. **é…ç½®éªŒè¯**ï¼šæ¯æ¬¡ä¿®æ”¹é…ç½®åéƒ½åº”è¿è¡Œ `npm run validate-config` éªŒè¯

---

**ä»»åŠ¡å®Œæˆæ ‡å¿—**ï¼š

- [ ] é…ç½®æ–‡ä»¶æ›´æ–°å®Œæˆ
- [ ] é…ç½®ç®¡ç†å™¨å®ç°å®Œæˆ
- [ ] éªŒè¯è„šæœ¬å®ç°å®Œæˆ
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] éªŒè¯è„šæœ¬è¿è¡Œæ­£å¸¸
